encoder:
  _target_: models.Encoder
  input_size: 21 
  hidden_size: 256
  latent_dim: 2 
  num_layers: 1
  dropout_lstm: 0
  dropout: 0.5
  bidirectional: False
decoder:
  _target_: models.Decoder
  input_size: ${model.encoder.input_size}
  hidden_size: ${model.encoder.hidden_size}
  latent_dim: ${model.encoder.latent_dim}
  window_size: ${data_preprocessor.window_size}
  num_layers: ${model.encoder.num_layers}
  bidirectional: ${model.encoder.bidirectional}
  dropout_lstm: 0
  dropout_layer: 0.5
rve:
  _target_: models.RVEAttention_MH
  attention_embed_dim: ${data_preprocessor.window_size}
  attention_num_heads: 2
  attention_dropout: 0.8
  batchnorm_dim: ${model.encoder.input_size}
  batchnorm_affine: False
  reconstruct: True
  dropout_regressor: 0.8
  regression_dims: 100